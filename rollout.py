from __future__ import unicode_literals, print_function
from __future__ import absolute_import
from __future__ import division
import tensorflow as tf
from tensorflow.python.ops import tensor_array_ops, control_flow_ops
# from tensorflow.python.ops import variable_scope
import numpy as np
from data import gen_vocab2dis_vocab
import data
PAD_TOKEN = "[PAD]"
STOP_DECODING = '[STOP]'
FLAGS = tf.app.flags.FLAGS


class Rollout(object):
    def __init__(self, generator, update_rate, decoder_scope):
        self.generator = generator
        self.update_rate = update_rate
        # TODO: for the variables update
        self._gen_hps = self.generator.hps
        self.g_embeddings = tf.identity(self.generator.embeddings)

        #######################################################################
        # placeholder definition

        self.summ = tf.placeholder(
            tf.int32, shape=[self._gen_hps.batch_size, self._gen_hps.max_dec_steps+1])
        self.cell_c = tf.placeholder(
            tf.float32, shape=[self._gen_hps.batch_size, self._gen_hps.hidden_dim])
        self.cell_h = tf.placeholder(
            tf.float32, shape=[self._gen_hps.batch_size, self._gen_hps.hidden_dim])
        # self.enc_states = tf.placeholder(
        #     tf.float32, shape=[self._gen_hps.batch_size, self._gen_hps.max_enc_steps,
        #                        self._gen_hps.hidden_dim])
        # this should be changed
        init_dec_in_state = tf.contrib.rnn.LSTMStateTuple(self.cell_c, self.cell_h)
        # sequence of tokens generated by generator
        self.given_num = tf.placeholder(tf.int32)

        # processed for batch
        self.emb_summ = tf.transpose(
            tf.nn.embedding_lookup(self.g_embeddings, self.summ), perm=[1, 0, 2])
        # seq_length x batch_size x emb_dim

        emb_summ_ar = tensor_array_ops.TensorArray(
            dtype=tf.float32, size=self._gen_hps.max_dec_steps + 1)
        emb_summ_ar = emb_summ_ar.unstack(self.emb_summ)

        summ_ar = tensor_array_ops.TensorArray(dtype=tf.int32, size=self._gen_hps.max_dec_steps + 1)
        summ_ar = summ_ar.unstack(tf.transpose(self.summ, perm=[1, 0]))
        ######################################################################

        self.gen_summ_ar = tensor_array_ops.TensorArray(
            dtype=tf.int32, size=self._gen_hps.max_dec_steps, dynamic_size=False, infer_shape=True)

        with tf.variable_scope(decoder_scope, reuse=True):
            def recurrence_given(i, dec_input, dec_in_state, given_num, gen_summ):
                next_input_id, new_state = self.generator.decode_onestep([dec_input], dec_in_state)
                next_input = emb_summ_ar.read(i)
                gen_summ = gen_summ.write(i-1, summ_ar.read(i))
                return i+1, next_input, new_state, given_num, gen_summ

            def recurrence_rollout(i, dec_input, dec_in_state, gen_summ):
                output_id, new_state = self.generator.decode_onestep([dec_input], dec_in_state)
                gen_summ = gen_summ.write(i-1, output_id)
                next_input_id_without_oovs = tf.where(
                    tf.less(output_id, self._gen_hps.gen_vocab_size),
                    output_id, tf.constant(
                        [self.generator._vocab.word2id(data.UNKNOWN_TOKEN)] * self._gen_hps.batch_size))
                next_input_emb = tf.nn.embedding_lookup(self.g_embeddings, next_input_id_without_oovs)
                return i+1, next_input_emb, new_state, gen_summ

            i, next_input, new_state, given_num, self.gen_summ_ar = control_flow_ops.while_loop(
                cond=lambda i, _1, _2, given_num, _4: i < given_num,
                body=recurrence_given,
                loop_vars=(tf.constant(1, dtype=tf.int32), emb_summ_ar.read(0),
                           init_dec_in_state, self.given_num, self.gen_summ_ar))

            _, _, _, self.gen_summ_ar = control_flow_ops.while_loop(
                cond=lambda i, _1, _2, _3: i < self._gen_hps.max_dec_steps+1,
                body=recurrence_rollout,
                loop_vars=(i, next_input, new_state, self.gen_summ_ar))

        self.gen_summ_ar = self.gen_summ_ar.stack()  # seq_length x batch_size
        self.gen_summ_ar = tf.transpose(self.gen_summ_ar, perm=[1, 0])
        # self.gen_summ_ar = tf.stop_gradient(self.gen_summ_ar)
        # batch_size x seq_length

    def get_reward(self, sess, gen_vocab, dis_vocab, source_batch,
                   enc_states, dec_in_state, k_samples,
                   rollout_num, discriminator):
        # dec_in_state is [batch_size, hidden_dim * 2] and that should be
        # changed to [batch_size, hidden_dim] for the attention_decoder

        article_oovs = source_batch.art_oovs
        art_words = source_batch.enc_batch_extend_vocab
        art_chars = gen_vocab2dis_vocab(
            art_words, gen_vocab, article_oovs,
            dis_vocab, discriminator.hps.max_enc_steps, PAD_TOKEN)
        # abs_chars = np.array(gen_vocab2dis_vocab(
        #     source_batch.target_batch, gen_vocab, article_oovs,
        #     dis_vocab, self._gen_hps.max_dec_steps, STOP_DECODING))
        k_rewards = []

        for samples in k_samples:
            rewards = []
            for i in range(rollout_num):
                for given_num in range(2, self._gen_hps.max_dec_steps+1):
                    feed_dict = {}
                    feed_dict[self.summ] = samples
                    # this is the source
                    feed_dict[self.generator.enc_lens] = source_batch.enc_lens
                    feed_dict[self.given_num] = given_num
                    feed_dict[self.generator.enc_states] = enc_states
                    feed_dict[self.generator.enc_padding_mask] = source_batch.enc_padding_mask
                    feed_dict[self.cell_c] = dec_in_state.c
                    feed_dict[self.cell_h] = dec_in_state.h
                    feed_dict[self.generator.enc_batch_extend_vocab] = art_words
                    # this is the source
                    feed_dict[self.generator.max_art_oovs] = source_batch.max_art_oovs
                    # how to deal with the coverage?

                    # the unique feature for the pointer gen is the
                    # enc_batch_extend_vocab and the max_art_oovs
                    rollout_samples_words = sess.run(self.gen_summ_ar, feed_dict)
                    # how about multiple generators for one discriminator?
                    rollout_samples_chars = gen_vocab2dis_vocab(
                        rollout_samples_words, gen_vocab, article_oovs,
                        dis_vocab, discriminator.hps.max_dec_steps, STOP_DECODING, art_words, print_sample=False)

                    feed = {
                        discriminator.inputs: rollout_samples_chars,
                        discriminator.conditions: art_chars}
                    ypred_for_auc = sess.run(discriminator.dis_ypred_for_auc, feed)
                    ypred = np.array([item[1] for item in ypred_for_auc])
                    if i == 0:
                        rewards.append(ypred)
                    else:
                        rewards[given_num - 2] += ypred

                # the last token reward
                samples_without_start = [s[1:].tolist() for s in samples]
                if i == 0:
                    ps = "beam_search in rollout"
                else:
                    ps = False
                samples_chars = gen_vocab2dis_vocab(
                    samples_without_start, gen_vocab, article_oovs,
                    dis_vocab, discriminator.hps.max_dec_steps, STOP_DECODING, print_sample=ps)
                feed = {
                    discriminator.inputs: samples_chars,
                    discriminator.conditions: art_chars}
                ypred_for_auc = sess.run(discriminator.dis_ypred_for_auc, feed)
                ypred = np.array([item[1] for item in ypred_for_auc])
                if i == 0:
                    rewards.append(ypred)
                else:
                    rewards[self._gen_hps.max_dec_steps - 1] += ypred

            k_rewards.append(np.transpose(np.array(rewards)) / (1.0 * rollout_num))
            # batch_size x seq_length

        return k_rewards
